{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.23.1-py2.py3-none-any.whl (8.9 MB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.0.212-py3-none-any.whl (1.1 MB)\n",
      "Collecting openai\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.4.0-cp39-cp39-win_amd64.whl (635 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.0.1-py3-none-any.whl (471 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting importlib-metadata<7,>=1.4 (from streamlit)\n",
      "  Using cached importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting numpy<2,>=1 (from streamlit)\n",
      "  Using cached numpy-1.25.0-cp39-cp39-win_amd64.whl (15.1 MB)\n",
      "Requirement already satisfied: packaging<24,>=14.1 in c:\\users\\vd003\\appdata\\roaming\\python\\python39\\site-packages (from streamlit) (21.3)\n",
      "Collecting pandas<3,>=0.25 (from streamlit)\n",
      "  Using cached pandas-2.0.2-cp39-cp39-win_amd64.whl (10.7 MB)\n",
      "Collecting pillow<10,>=6.2.0 (from streamlit)\n",
      "  Using cached Pillow-9.5.0-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-4.23.3-cp39-cp39-win_amd64.whl (422 kB)\n",
      "Collecting pyarrow>=4.0 (from streamlit)\n",
      "  Using cached pyarrow-12.0.1-cp39-cp39-win_amd64.whl (21.5 MB)\n",
      "Collecting pympler<2,>=0.9 (from streamlit)\n",
      "  Using cached Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2 in c:\\users\\vd003\\appdata\\roaming\\python\\python39\\site-packages (from streamlit) (2.8.2)\n",
      "Collecting requests<3,>=2.4 (from streamlit)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting rich<14,>=10.11.0 (from streamlit)\n",
      "  Using cached rich-13.4.2-py3-none-any.whl (239 kB)\n",
      "Collecting tenacity<9,>=8.0.0 (from streamlit)\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting toml<2 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting typing-extensions<5,>=4.0.1 (from streamlit)\n",
      "  Using cached typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
      "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
      "  Using cached tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Using cached validators-0.20.0.tar.gz (30 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting gitpython!=3.1.19,<4,>=3 (from streamlit)\n",
      "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Collecting pydeck<1,>=0.1.dev5 (from streamlit)\n",
      "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\vd003\\appdata\\roaming\\python\\python39\\site-packages (from streamlit) (6.2)\n",
      "Collecting watchdog (from streamlit)\n",
      "  Using cached watchdog-3.0.0-py3-none-win_amd64.whl (82 kB)\n",
      "Collecting PyYAML>=5.4.1 (from langchain)\n",
      "  Using cached PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.17-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
      "Collecting langchainplus-sdk>=0.0.17 (from langchain)\n",
      "  Using cached langchainplus_sdk-0.0.17-py3-none-any.whl (25 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain)\n",
      "  Using cached numexpr-2.8.4-cp39-cp39-win_amd64.whl (92 kB)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting pydantic<2,>=1 (from langchain)\n",
      "  Using cached pydantic-1.10.9-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Collecting tqdm (from openai)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2023.6.3-cp39-cp39-win_amd64.whl (268 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached charset_normalizer-3.1.0-cp39-cp39-win_amd64.whl (97 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.2-cp39-cp39-win_amd64.whl (61 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vd003\\appdata\\roaming\\python\\python39\\site-packages (from click<9,>=7.0->streamlit) (0.4.5)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit)\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7,>=1.4->streamlit)\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vd003\\appdata\\roaming\\python\\python39\\site-packages (from packaging<24,>=14.1->streamlit) (3.0.9)\n",
      "Collecting pytz>=2020.1 (from pandas<3,>=0.25->streamlit)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas<3,>=0.25->streamlit)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vd003\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil<3,>=2->streamlit) (1.16.0)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.4->streamlit)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.4->streamlit)\n",
      "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.4->streamlit)\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.11.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich<14,>=10.11.0->streamlit)\n",
      "  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-2.0.2-cp39-cp39-win_amd64.whl (192 kB)\n",
      "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\vd003\\appdata\\roaming\\python\\python39\\site-packages (from validators<1,>=0.2->streamlit) (5.1.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit)\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
      "  Using cached MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached pyrsistent-0.19.3-cp39-cp39-win_amd64.whl (62 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Building wheels for collected packages: validators\n",
      "  Building wheel for validators (pyproject.toml): started\n",
      "  Building wheel for validators (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19590 sha256=4daeb9dafd96e4cb62cabb0a62f6c999d6b554738788ca6b9e3365eee3dc4737\n",
      "  Stored in directory: c:\\users\\vd003\\appdata\\local\\pip\\cache\\wheels\\2d\\f0\\a8\\1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
      "Successfully built validators\n",
      "Installing collected packages: pytz, zipp, watchdog, validators, urllib3, tzdata, typing-extensions, tqdm, toolz, toml, tenacity, smmap, regex, PyYAML, pyrsistent, pympler, pygments, protobuf, pillow, numpy, mypy-extensions, multidict, mdurl, MarkupSafe, idna, greenlet, frozenlist, click, charset-normalizer, certifi, cachetools, blinker, attrs, async-timeout, yarl, typing-inspect, SQLAlchemy, requests, pytz-deprecation-shim, pydantic, pyarrow, pandas, numexpr, marshmallow, markdown-it-py, jsonschema, jinja2, importlib-metadata, gitdb, aiosignal, tzlocal, tiktoken, rich, pydeck, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, gitpython, altair, aiohttp, streamlit, openai, dataclasses-json, langchain\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.12.0\n",
      "    Uninstalling Pygments-2.12.0:\n",
      "      Successfully uninstalled Pygments-2.12.0\n",
      "Successfully installed MarkupSafe-2.1.3 PyYAML-6.0 SQLAlchemy-2.0.17 aiohttp-3.8.4 aiosignal-1.3.1 altair-5.0.1 async-timeout-4.0.2 attrs-23.1.0 blinker-1.6.2 cachetools-5.3.1 certifi-2023.5.7 charset-normalizer-3.1.0 click-8.1.3 dataclasses-json-0.5.8 frozenlist-1.3.3 gitdb-4.0.10 gitpython-3.1.31 greenlet-2.0.2 idna-3.4 importlib-metadata-6.7.0 jinja2-3.1.2 jsonschema-4.17.3 langchain-0.0.212 langchainplus-sdk-0.0.17 markdown-it-py-3.0.0 marshmallow-3.19.0 marshmallow-enum-1.5.1 mdurl-0.1.2 multidict-6.0.4 mypy-extensions-1.0.0 numexpr-2.8.4 numpy-1.25.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pandas-2.0.2 pillow-9.5.0 protobuf-4.23.3 pyarrow-12.0.1 pydantic-1.10.9 pydeck-0.8.1b0 pygments-2.15.1 pympler-1.0.1 pyrsistent-0.19.3 pytz-2023.3 pytz-deprecation-shim-0.1.0.post0 regex-2023.6.3 requests-2.31.0 rich-13.4.2 smmap-5.0.0 streamlit-1.23.1 tenacity-8.2.2 tiktoken-0.4.0 toml-0.10.2 toolz-0.12.0 tqdm-4.65.0 typing-extensions-4.6.3 typing-inspect-0.9.0 tzdata-2023.3 tzlocal-4.3.1 urllib3-2.0.3 validators-0.20.0 watchdog-3.0.0 yarl-1.9.2 zipp-3.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 14:14:07.134 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\vd003\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def generate_response(uploaded_file, openai_api_key, query_text):\n",
    "    # Load document if file is uploaded\n",
    "    if uploaded_file is not None:\n",
    "        documents = [uploaded_file.read().decode()]\n",
    "        # Split documents into chunks\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        texts = text_splitter.create_documents(documents)\n",
    "        # Select embeddings\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        # Create a vectorstore from documents\n",
    "        db = Chroma.from_documents(texts, embeddings)\n",
    "        # Create retriever interface\n",
    "        retriever = db.as_retriever()\n",
    "        # Create QA chain\n",
    "        qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type='stuff', retriever=retriever)\n",
    "        return qa.run(query_text)\n",
    "\n",
    "# Page title\n",
    "st.set_page_config(page_title='🦜🔗 Ask the Doc App')\n",
    "st.title('🦜🔗 Ask the Doc App')\n",
    "\n",
    "# File upload\n",
    "uploaded_file = st.file_uploader('Upload an article', type='txt')\n",
    "# Query text\n",
    "query_text = st.text_input('Enter your question:', placeholder = 'Please provide a short summary.', disabled=not uploaded_file)\n",
    "\n",
    "# Form input and query\n",
    "result = []\n",
    "with st.form('myform', clear_on_submit=True):\n",
    "    openai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))\n",
    "    submitted = st.form_submit_button('Submit', disabled=not(uploaded_file and query_text))\n",
    "    if submitted and openai_api_key.startswith('sk-'):\n",
    "        with st.spinner('Calculating...'):\n",
    "            response = generate_response(uploaded_file, openai_api_key, query_text)\n",
    "            result.append(response)\n",
    "            del openai_api_key\n",
    "\n",
    "if len(result):\n",
    "    st.info(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(uploaded_file, openai_api_key, query_text):\n",
    "    # Load document if file is uploaded\n",
    "    if uploaded_file is not None:\n",
    "        documents = [uploaded_file.read().decode()]\n",
    "    # Split documents into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.create_documents(documents)\n",
    "    # Select embeddings\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    # Create a vectorstore from documents\n",
    "    db = Chroma.from_documents(texts, embeddings)\n",
    "    # Create retriever interface\n",
    "    retriever = db.as_retriever()\n",
    "    # Create QA chain\n",
    "    qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type='stuff', retriever=retriever)\n",
    "    return qa.run(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_form_data=FormData(form_id='myform'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.set_page_config(page_title='🦜🔗 Ask the Doc App')\n",
    "st.title('🦜🔗 Ask the Doc App')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File upload\n",
    "uploaded_file = st.file_uploader('Upload an article', type='txt')\n",
    "# Query text\n",
    "query_text = st.text_input('Enter your question:', placeholder = 'Please provide a short summary.', disabled=not uploaded_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form input and query\n",
    "result = []\n",
    "with st.form('myform', clear_on_submit=True):\n",
    "    openai_api_key = st.text_input('OpenAI API Key', type='password', disabled=not (uploaded_file and query_text))\n",
    "    submitted = st.form_submit_button('Submit', disabled=not(uploaded_file and query_text))\n",
    "    if submitted and openai_api_key.startswith('sk-'):\n",
    "        with st.spinner('Calculating...'):\n",
    "            response = generate_response(uploaded_file, openai_api_key, query_text)\n",
    "            result.append(response)\n",
    "            del openai_api_key\n",
    "\n",
    "if len(result):\n",
    "    st.info(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
